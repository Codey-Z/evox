{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidevice Algorithm\n",
    "\n",
    "This guide will show you how to write algorithms that can run on multiple devices (multiple GPUs) in EvoX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from evox import dataclass, pytree_field, problems, workflows, monitors, algorithms, use_state\n",
    "from evox.core.distributed import ShardingType\n",
    "from evox.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we consider the following simple setup:\n",
    "```\n",
    "    Node1\n",
    "      |\n",
    " +----+----+\n",
    " |         |\n",
    "GPU       GPU\n",
    "```\n",
    "Where we only have one node with multiple GPUs. The communication between the GPUs is done through the PCIe or NVLink.\n",
    "When running in a distributed setup, we need to make decisions on how to place the data on these GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the vanilla PSO algorithm as an example. In PSO, each GPU can independently update the local information for its particles. On the other hand, updating the global information requires communication between GPUs, but this process can be handled rather efficiently using an all-reduce operation.\n",
    "\n",
    "Here is an illustration of population in PSO, it has two dimensions: the number of particles (population size) and the problem dimension.\n",
    "```\n",
    "  Problem Dimension\n",
    "+-------------------+\n",
    "|                   |\n",
    "|  GPU 0            |  Population Size\n",
    "|  All particles    |\n",
    "|                   |\n",
    "+-------------------+\n",
    "```\n",
    "After sharding it across the population dimension, we have the following:\n",
    "```\n",
    "  Problem Dimension\n",
    "+-------------------+\n",
    "|                   |\n",
    "|  GPU 0            |  Population Size / 2\n",
    "+-------------------+\n",
    "|                   |\n",
    "|  GPU 1            |  Population Size / 2\n",
    "+-------------------+\n",
    "```\n",
    "Similarly, we can also shard the velocity variable.\n",
    "This will reduce the memory usage on each GPU by half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only change:\n",
    "# Add the sharding metadata\n",
    "@dataclass\n",
    "class SpecialPSOState:\n",
    "    population: jax.Array = pytree_field(sharding=ShardingType.SHARED_FIRST_DIM)\n",
    "    velocity: jax.Array = pytree_field(sharding=ShardingType.SHARED_FIRST_DIM)\n",
    "    fitness: jax.Array = pytree_field(sharding=ShardingType.SHARED_FIRST_DIM)\n",
    "    local_best_location: jax.Array = pytree_field(sharding=ShardingType.SHARED_FIRST_DIM)\n",
    "    local_best_fitness: jax.Array = pytree_field(sharding=ShardingType.SHARED_FIRST_DIM)\n",
    "    global_best_location: jax.Array\n",
    "    global_best_fitness: jax.Array\n",
    "    key: jax.random.PRNGKey\n",
    "\n",
    "\n",
    "# inherit from the base PSO algorithm\n",
    "# and replace the State type with SpecialPSOState, which contains the sharding metadata\n",
    "@dataclass\n",
    "class PSO(algorithms.PSO):\n",
    "    def setup(self, key):\n",
    "        state_key, init_pop_key, init_v_key = jax.random.split(key, 3)\n",
    "        length = self.ub - self.lb\n",
    "        population = jax.random.uniform(\n",
    "            init_pop_key, shape=(self.pop_size, self.dim)\n",
    "        )\n",
    "        population = population * length + self.lb\n",
    "        velocity = jax.random.uniform(init_v_key, shape=(self.pop_size, self.dim))\n",
    "        velocity = velocity * length * 2 - length\n",
    "\n",
    "        return SpecialPSOState(\n",
    "            population=population,\n",
    "            velocity=velocity,\n",
    "            fitness=jnp.full((self.pop_size,), jnp.inf),\n",
    "            local_best_location=population,\n",
    "            local_best_fitness=jnp.full((self.pop_size,), jnp.inf),\n",
    "            global_best_location=population[0],\n",
    "            global_best_fitness=jnp.array([jnp.inf]),\n",
    "            key=state_key,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pso = PSO(\n",
    "    lb=jnp.full(shape=(2,), fill_value=-32),\n",
    "    ub=jnp.full(shape=(2,), fill_value=32),\n",
    "    pop_size=100,\n",
    ")\n",
    "ackley = problems.numerical.Ackley()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = monitors.EvalMonitor()\n",
    "workflow = workflows.StdWorkflow(\n",
    "    pso,\n",
    "    ackley,\n",
    "    monitors=[monitor],\n",
    ")\n",
    "key = jax.random.PRNGKey(42)\n",
    "state = workflow.init(key)\n",
    "state = workflow.enable_multi_devices(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "State(StdWorkflowState(generation=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec(), memory_kind=device), first_step=True), {'algorithm': State(SpecialPSOState(population=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec('POP',), memory_kind=device), velocity=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec('POP',), memory_kind=device), fitness=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec('POP',), memory_kind=device), local_best_location=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec('POP',), memory_kind=device), local_best_fitness=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec('POP',), memory_kind=device), global_best_location=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec(), memory_kind=device), global_best_fitness=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec(), memory_kind=device), key=NamedSharding(mesh=Mesh('POP': 2), spec=PartitionSpec(), memory_kind=device)), {}),'monitors0': State(EvalMonitorState(first_step=True, latest_solution=None, latest_fitness=None, topk_solutions=None, topk_fitness=None), {}),'problem': State({}, {})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the state is correctly sharded\n",
    "jax.tree.map(lambda x: x.sharding, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the workflow for 50 steps\n",
    "for i in range(50):\n",
    "    state = workflow.step(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solution, _state = use_state(monitor.get_best_solution)(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0002041  -0.00019218]\n"
     ]
    }
   ],
   "source": [
    "print(best_solution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
