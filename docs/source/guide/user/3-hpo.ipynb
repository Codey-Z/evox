{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient HPO with EvoX\n",
    "\n",
    "In this chapter, we will explore how to use EvoX for hyperparameter optimization (HPO).\n",
    "\n",
    "HPO plays a crucial role in many machine learning tasks but is often overlooked due to its high computational cost, which can sometimes take days to process, as well as the challenges involved in deployment.\n",
    "\n",
    "With EvoX, we can simplify HPO deployment using the [HPOProblemWrapper](#HPOProblemWrapper) and achieve efficient computation by leveraging the vmap method and GPU acceleration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Workflow into Problem\n",
    "\n",
    "```{image} /_static/HPO_structure.svg\n",
    ":alt: HPO structure\n",
    ":width: 600px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The key to deploying HPO with EvoX is to transform the [workflows](#evox.workflows) into [problems](#evox.problems) using the [HPOProblemWrapper](#HPOProblemWrapper). Once transformed, we can treat the [workflows](#evox.workflows) as standard [problems](#evox.problems). The input to the 'HPO problem' consists of the hyperparameters, and the output is the evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Component -- `HPOProblemWrapper`\n",
    "\n",
    "To ensure the [HPOProblemWrapper](#HPOProblemWrapper) recognizes the hyperparameters, we need to wrap them using [Parameter](#Parameter). With this straightforward step, the hyperparameters will be automatically identified.\n",
    "\n",
    "```python\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self,...): \n",
    "        self.omega = Parameter([1.0, 2.0]) # wrap the hyperparameters with `Parameter`\n",
    "        self.beta = Parameter(0.1)\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        # run algorithm step depending on the value of self.omega and self.beta\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the `HPOFitnessMonitor`\n",
    "\n",
    "We provide an [HPOFitnessMonitor](#HPOFitnessMonitor) that supports calculating 'IGD' and 'HV' metrics for multi-objective problems, as well as the minimum value for single-objective problems.\n",
    "\n",
    "It is important to note that the [HPOFitnessMonitor](#HPOFitnessMonitor) is a basic monitor designed for HPO problems. You can also create your own customized monitor flexibly using the approach outlined in [Deploy HPO with Custom Algorithms](#/guide/developer/custom_hpo_prob)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Here, we'll demonstrate a simple example of using EvoX for HPO. Specifically, we will use the [PSO](#PSO) algorithm to optimize the hyperparameters of the [PSO](#PSO) algorithm for solving the sphere problem.\n",
    "\n",
    "Please note that this chapter provides only a brief overview of HPO deployment. For a more detailed guide, refer to [Deploy HPO with Custom Algorithms](#/guide/developer/custom_hpo_prob).\n",
    "\n",
    "To start, let's import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from evox.algorithms.pso_variants.pso import PSO\n",
    "from evox.core import Problem, jit_class\n",
    "from evox.problems.hpo_wrapper import HPOFitnessMonitor, HPOProblemWrapper\n",
    "from evox.workflows import EvalMonitor, StdWorkflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a simple sphere problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_class\n",
    "class Sphere(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def evaluate(self, x: torch.Tensor):\n",
    "        return (x * x).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the [StdWorkflow](#StdWorkflow) to wrap the [problem](#evox.problems), [algorithm](#evox.algorithms) and [monitor](#Monitor). Then we use the [HPOProblemWrapper](#HPOProblemWrapper) to transform the [StdWorkflow](#StdWorkflow) to an HPO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inner_algo = PSO(10, -10 * torch.ones(10), 10 * torch.ones(10))\n",
    "inner_prob = Sphere()\n",
    "inner_monitor = HPOFitnessMonitor()\n",
    "inner_monitor.setup()\n",
    "inner_workflow = StdWorkflow()\n",
    "inner_workflow.setup(inner_algo, inner_prob, monitor = inner_monitor)\n",
    "# Transform the inner workflow to an HPO problem\n",
    "hpo_prob = HPOProblemWrapper(iterations = 15, num_instances = 5, workflow = inner_workflow, copy_init_state = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [HPOProblemWrapper](#HPOProblemWrapper) takes 4 arguments:\n",
    "1. iterations: The number of iterations to be executed in the optimization process.\n",
    "2. num_instances: The number of instances to be executed in parallel in the optimization process.\n",
    "3. workflow: The workflow to be used in the optimization process. Must be wrapped by [jit_class](#jit_class).\n",
    "4. copy_init_state: Whether to copy the initial state of the workflow for each evaluation. Defaults to `True`. If your workflow contains operations that IN-PLACE modify the tensor(s) in initial state, this should be set to `True`. Otherwise, you can set it to `False` to save memory.\n",
    "\n",
    "We can verify whether the [HPOProblemWrapper](#HPOProblemWrapper) correctly recognizes the hyperparameters we define. Since no modifications are made to the hyperparameters across the 5 instances, they should remain identical for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init params:\n",
      " {'self.algorithm.w': Parameter containing:\n",
      "tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000], device='cuda:0'), 'self.algorithm.phi_p': Parameter containing:\n",
      "tensor([2.5000, 2.5000, 2.5000, 2.5000, 2.5000], device='cuda:0'), 'self.algorithm.phi_g': Parameter containing:\n",
      "tensor([0.8000, 0.8000, 0.8000, 0.8000, 0.8000], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "params = hpo_prob.get_init_params()\n",
    "print('init params:\\n',params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a custom set of hyperparameter values. It is important to ensure that the number of hyperparameter sets matches the number of instances in the [HPOProblemWrapper](#HPOProblemWrapper). Additionally, custom hyperparameters must be provided as a dictionary and wrapped using the [Parameter](#Parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self.algorithm.w': Parameter containing:\n",
      "tensor([[0.3606],\n",
      "        [0.8700],\n",
      "        [0.8371],\n",
      "        [0.9362],\n",
      "        [0.6158]], device='cuda:0'), 'self.algorithm.phi_p': Parameter containing:\n",
      "tensor([[0.1089],\n",
      "        [0.8496],\n",
      "        [0.2948],\n",
      "        [0.4693],\n",
      "        [0.5615]], device='cuda:0'), 'self.algorithm.phi_g': Parameter containing:\n",
      "tensor([[0.6890],\n",
      "        [0.1214],\n",
      "        [0.7578],\n",
      "        [0.8836],\n",
      "        [0.6416]], device='cuda:0')}\n",
      "result:\n",
      " tensor([104.5386, 125.3063,  45.4021, 159.3124,  27.9212], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "params = hpo_prob.get_init_params()\n",
    "# since we have 5 instances, we need to pass 5 sets of hyperparameters\n",
    "params[\"self.algorithm.w\"] = torch.nn.Parameter(torch.rand(5, 1), requires_grad=False)\n",
    "params[\"self.algorithm.phi_p\"] = torch.nn.Parameter(torch.rand(5, 1), requires_grad=False)\n",
    "params[\"self.algorithm.phi_g\"] = torch.nn.Parameter(torch.rand(5, 1), requires_grad=False)\n",
    "result = hpo_prob.evaluate(params)\n",
    "print(params)\n",
    "print('result:\\n',result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the [PSO](#PSO) algorithm to optimize the hyperparameters of the [PSO](#PSO) algorithm.\n",
    "\n",
    "It is important to ensure that the population size of the [PSO](#PSO) matches the number of instances; otherwise, unexpected errors may occur.\n",
    "\n",
    "Additionally, the solution needs to be transformed in the outer workflow, as the [HPOProblemWrapper](#HPOProblemWrapper) requires the input to be in the form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      " tensor([[0.3746, 2.1251, 1.0888]], device='cuda:0') \n",
      "\n",
      "result:\n",
      " tensor([1.9542], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class solution_transform(torch.nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return {\"self.algorithm.w\": x[:,0],\n",
    "                \"self.algorithm.phi_p\": x[:,1],\n",
    "                \"self.algorithm.phi_g\": x[:,2],\n",
    "                }\n",
    "\n",
    "outer_algo = PSO(5, 0 * torch.ones(3), 3 * torch.ones(3))\n",
    "monitor = EvalMonitor(full_sol_history = False)\n",
    "outer_workflow = StdWorkflow()\n",
    "outer_workflow.setup(outer_algo, hpo_prob, monitor = monitor, solution_transform = solution_transform())\n",
    "outer_workflow.init_step()\n",
    "for _ in range(20):\n",
    "    outer_workflow.step()\n",
    "monitor = outer_workflow.get_submodule(\"monitor\")\n",
    "print('params:\\n', monitor.topk_solutions, '\\n')\n",
    "print('result:\\n', monitor.topk_fitness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
