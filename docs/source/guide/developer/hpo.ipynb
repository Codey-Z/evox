{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient HPO with EvoX\n",
    "\n",
    "In this chapter, we will discuss how to use EvoX for hyperparameter optimization (HPO).\n",
    "\n",
    "HPO is an essential step in many machine learning tasks, yet it often goes underappreciated. This is mainly due to its heavy computational demands, which can sometimes require days of processing, as well as the difficulties involved in deployment.\n",
    "\n",
    "In EvoX, we can easily deploy HPO using the [HPOProblemWrapper](#HPOProblemWrapper), and achieve efficient computation by leveraging the `vmap` method and GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming Workflow into Problem\n",
    "\n",
    "```{image} /_static/HPO_structure.svg\n",
    ":alt: HPO structure\n",
    ":width: 600px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The key to deploy HPO with EvoX is to transform the [workflows](#evox.workflows) into [problems](#evox.problems) with the [HPOProblemWrapper](#HPOProblemWrapper). After that, we can treat [workflows](#evox.workflows) as common [problems](#evox.problems). The input of the 'HPO problem' are the hyperparameters and the output is the metric.\n",
    "\n",
    "### The Key Component -- HPOProblemWrapper\n",
    "To enable [HPOProblemWrapper](#HPOProblemWrapper) to recognize the hyperparameters, we need to wrap the hyperparameters with [Parameter](#Parameter). After this simple operation, the hyperparameters will be automatically recognized.\n",
    "\n",
    "```python\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self,...):\n",
    "        self.omega = Parameter([1.0, 2.0]) # wrap the hyperparameters with `Parameter`\n",
    "        self.beta = Parameter(0.1)\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        # run algorithm step depending on the value of self.omega and self.beta\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Algorithms Parallelizable\n",
    "\n",
    "In order to make the 'inner algorithm' parallelizable, we may need to make some modifications to the algorithm. To enable the function to be JIT-compilable, it must satisfy certain conditions. In addition to these conditions, we also require the algorithm to meet the following two constraints:\n",
    "1. The algorithm should have no methods with in-place operations on the attributes of the algotirhm itself.\n",
    "\n",
    "```python\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self,...):\n",
    "        self.pop = torch.rand(10,10) #attribute of the algotirhm itself\n",
    "        pass\n",
    "\n",
    "    def step_in_place(self): # method with in-place operations\n",
    "        self.pop.copy_(pop)\n",
    "        pass\n",
    "\n",
    "    def step_out_of_place(self): # method without in-place operations\n",
    "        self.pop = pop\n",
    "        pass\n",
    "```\n",
    "\n",
    "2. The code logic does not rely on python control flow.\n",
    "\n",
    "```python\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self,...):\n",
    "        self.pop = rand(10,10) #attribute of the algotirhm itself\n",
    "        pass\n",
    "\n",
    "    def plus(self, y):\n",
    "        self.pop += y\n",
    "        pass\n",
    "\n",
    "    def minus(self, y):\n",
    "        self.pop -= y\n",
    "        pass\n",
    "\n",
    "    def step_with_python_control_flow(self, y): # function with python control flow\n",
    "        x = rand()\n",
    "        if x>0.5:\n",
    "            self.plus(y)\n",
    "        else:\n",
    "            self.minus(y)\n",
    "        pass\n",
    "\n",
    "    def step_without_python_control_flow(self, y): # function without python control flow\n",
    "        x = rand()\n",
    "        cond = x > 0.5\n",
    "        _if_else_ = TracingCond(self.plus, self.minus)\n",
    "        _if_else_.cond(cond,y)\n",
    "        self.pop = pop\n",
    "        pass\n",
    "```\n",
    "\n",
    "In EvoX, we can easily make the algorithm parallelizable by the [trace_impl](#trace_impl) decorator.\n",
    "\n",
    "The parameter of this decorator is a non-parallelizable function, and the decorated function is a rewrite of the original function.\n",
    "\n",
    "Under this mechanism, we can retain the original function for use outside HPO tasks while enabling efficient computation within HPO tasks. Moreover, this modification is highly convenient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the HPOMonitor\n",
    "\n",
    "We should use [HPOMonitor](#HPOMonitor) in the HPO task to monitor the metric of each inner algorithm. The HPOMonitor only add one `tell_fitness` method comparing with the common [monitor](#Monitor). This is designed to make the evaluation metrics for a set of hyperparameters more flexible, as metrics in HPO tasks are often multi-dimensional and complex.\n",
    "\n",
    "Users only need to create a subclass of HPOMonitor and override the `tell_fitness` method to define their own evaluation metrics.\n",
    "\n",
    "We also provide a simple [HPOFitnessMonitor](#HPOFitnessMonitor), which supports calculating 'IGD' and 'HV' metrics for multi-objective problems and minimum value for single-objective problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "Here we would show you a simple example of how to use HPO with EvoX. We will use the [PSO](#PSO) algorithm to search for the best hyperparameters of a simple algorithm to solve the sphere problem.\n",
    "\n",
    "First, we need to import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from evox.algorithms.pso_variants.pso import PSO\n",
    "from evox.core import Algorithm, Mutable, Parameter, Problem, jit_class, trace_impl\n",
    "from evox.problems.hpo_wrapper import HPOFitnessMonitor, HPOProblemWrapper\n",
    "from evox.utils import TracingCond\n",
    "from evox.workflows import EvalMonitor, StdWorkflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define an simple sphere problem. Note that this has no difference from the common [problems](#evox.problems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_class\n",
    "class Sphere(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def evaluate(self, x: torch.Tensor):\n",
    "        return (x * x).sum(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can define the algorithm. The oringinal `step` function is non-parallelizable. We rewrite it with the decorator [trace_impl](#trace_impl) to be parallelizable. We modify the in-place opeartions and python control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit_class\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self, pop_size: int, lb: torch.Tensor, ub: torch.Tensor):\n",
    "        super().__init__()\n",
    "        assert lb.ndim == 1 and ub.ndim == 1, f\"Lower and upper bounds shall have ndim of 1, got {lb.ndim} and {ub.ndim}\"\n",
    "        assert lb.shape == ub.shape, f\"Lower and upper bounds shall have same shape, got {lb.ndim} and {ub.ndim}\"\n",
    "        self.pop_size = pop_size\n",
    "        self.hp = Parameter([1.0, 2.0, 3.0, 4.0])  # the hyperparameters to be optimized\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.dim = lb.shape[0]\n",
    "        self.pop = Mutable(torch.empty(self.pop_size, lb.shape[0], dtype=lb.dtype, device=lb.device))\n",
    "        self.fit = Mutable(torch.empty(self.pop_size, dtype=lb.dtype, device=lb.device))\n",
    "\n",
    "    def strategy_1(self, pop):  # one update strategy\n",
    "        pop = pop * (self.hp[0] + self.hp[1])\n",
    "\n",
    "    def strategy_2(self, pop):  #  the other update strategy\n",
    "        pop = pop * (self.hp[2] + self.hp[3])\n",
    "\n",
    "    def step(self):\n",
    "        pop = torch.rand(self.pop_size, self.dim, dtype=self.lb.dtype, device=self.lb.device)  # simply random sampling\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        control_number = torch.rand()\n",
    "        if control_number < 0.5:  # conditional control\n",
    "            pop = self.strategy_1(pop)\n",
    "        else:\n",
    "            pop = self.strategy_2(pop)\n",
    "        self.pop.copy_(pop)  # in-place update\n",
    "        self.fit.copy_(self.evaluate(pop))\n",
    "\n",
    "    @trace_impl(step)  # rewrite the step function to support vmap\n",
    "    def trace_step(self):\n",
    "        pop = torch.rand(self.pop_size, self.dim, dtype=self.lb.dtype, device=self.lb.device)\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        pop = pop * self.hp[0]\n",
    "        control_number = torch.rand()\n",
    "        cond = control_number < 0.5\n",
    "        _if_else_ = TracingCond(self.strategy_1, self.strategy_2)\n",
    "        _if_else_.cond(cond, pop)\n",
    "        self.pop = pop\n",
    "        self.fit = self.evaluate(pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the [StdWorkflow](#StdWorkflow) to wrap the problem, algorithm and monitor. Then we use the [HPOProblemWrapper](#HPOProblemWrapper) to transform the StdWorkflow to an HPO problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inner_algo = ExampleAlgorithm(10, -10 * torch.ones(8), 10 * torch.ones(8))\n",
    "inner_prob = Sphere()\n",
    "inner_monitor = HPOFitnessMonitor()\n",
    "inner_monitor.setup()\n",
    "inner_workflow = StdWorkflow()\n",
    "inner_workflow.setup(inner_algo, inner_prob, monitor=inner_monitor)\n",
    "# Transform the inner workflow to an HPO problem\n",
    "hpo_prob = HPOProblemWrapper(iterations=9, num_instances=7, workflow=inner_workflow, copy_init_state=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [HPOProblemWrapper](#HPOProblemWrapper) takes 4 arguments:\n",
    "1. iterations: The number of iterations to be executed in the optimization process.\n",
    "2. num_instances: The number of instances to be executed in parallel in the optimization process.\n",
    "3. workflow: The workflow to be used in the optimization process. Must be wrapped by [jit_class](#jit_class).\n",
    "4. copy_init_state: Whether to copy the initial state of the workflow for each evaluation. Defaults to `True`. If your workflow contains operations that IN-PLACE modify the tensor(s) in initial state, this should be set to `True`. Otherwise, you can set it to `False` to save memory.\n",
    "\n",
    "We can test whether the HPOProblemWrapper recognizes the hyperparameters we define. Since we make no modification to the hyperparameters for the 7 instances, the hyperparameters should be the same for all instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init params:\n",
      " {'self.algorithm.hp': Parameter containing:\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [1., 2., 3., 4.]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "params = hpo_prob.get_init_params()\n",
    "print(\"init params:\\n\", params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify a set of hyperparameter values ourselves. Note that the number of hyperparameter sets must be consistent with the number of instances in the HPOProblemWrapper and the custom hyperparameters must be passed in the form of a dictionary and wrapped using the [Parameter](#Parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      " {'self.algorithm.hp': Parameter containing:\n",
      "tensor([[0.6151, 0.4238, 0.3006, 0.0424],\n",
      "        [0.4175, 0.8058, 0.8549, 0.5446],\n",
      "        [0.2170, 0.1184, 0.5396, 0.5979],\n",
      "        [0.5893, 0.6488, 0.3352, 0.4294],\n",
      "        [0.0289, 0.7860, 0.1178, 0.5868],\n",
      "        [0.8950, 0.0965, 0.4080, 0.5824],\n",
      "        [0.9049, 0.5265, 0.1892, 0.5064]], device='cuda:0')} \n",
      "\n",
      "result:\n",
      " tensor([32.5368, 13.1591,  2.6436, 24.7213,  0.0959, 38.7169, 77.4396],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "params = hpo_prob.get_init_params()\n",
    "# since we have 7 instances, we need to pass 7 sets of hyperparameters\n",
    "params[\"self.algorithm.hp\"] = torch.nn.Parameter(torch.rand(7, 4), requires_grad=False)\n",
    "result = hpo_prob.evaluate(params)\n",
    "print(\"params:\\n\", params, \"\\n\")\n",
    "print(\"result:\\n\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the [PSO](#PSO) algorithm to optimize the hyperparameters of ExampleAlgorithm. Note that the population size of the PSO should match the number of instances; otherwise, unexpected errors may occur. Here, we need to transform the solution in the outer workflow, as the HPOProblemWrapper must accept a dictionary as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params:\n",
      " tensor([[-3.1316e-04, -4.2392e-01, -2.0663e+00,  2.8299e+00]], device='cuda:0') \n",
      "\n",
      "result:\n",
      " tensor([9.0117e-06], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class solution_transform(torch.nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return {\"self.algorithm.hp\": x}\n",
    "\n",
    "\n",
    "outer_algo = PSO(7, -3 * torch.ones(4), 3 * torch.ones(4))\n",
    "monitor = EvalMonitor(full_sol_history=False)\n",
    "outer_workflow = StdWorkflow()\n",
    "outer_workflow.setup(outer_algo, hpo_prob, monitor=monitor, solution_transform=solution_transform())\n",
    "outer_workflow.init_step()\n",
    "for _ in range(20):\n",
    "    outer_workflow.step()\n",
    "monitor = outer_workflow.get_submodule(\"monitor\")\n",
    "print(\"params:\\n\", monitor.topk_solutions, \"\\n\")\n",
    "print(\"result:\\n\", monitor.topk_fitness)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
