{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing `@trace_impl` and `@vmap_impl`\n",
    "\n",
    "When designing a function or method, you may not always consider whether it is `JIT`-compatible. However, this property becomes crucial in specific scenarios, such as solving Hyperparameter Optimization (HPO) problems. For more details on deploying HPO with EvoX, refer to [Efficient HPO with EvoX](#/guide/user/3-hpo).\n",
    "\n",
    "A typical characteristic of such problems is that only certain parts of the algorithm need modificationâ€”for instance, the `step` method of an algorithm. This allows you to avoid rewriting the entire algorithm. In such cases, you can use the `@trace_impl` or `@vmap_impl` decorator to rewrite the function as a trace-JIT-time or vmap-JIT-time proxy for the specified `target` method.\n",
    "\n",
    "The decorators [`@trace_impl`](#trace_impl) and [`@vmap_impl`](#vmap_impl) accept a single input parameter: the target method invoked when not tracing JIT. These decorators are applicable only to member methods within a `jit_class`.\n",
    "\n",
    "Since the annotated function serves as a rewritten version of the target function, it must maintain identical input/output signatures (e.g., number and types of arguments). Otherwise, the resulting behavior is undefined.\n",
    "\n",
    "If the annotated function is intended for use with `vmap`, it must satisfy three additional constraints:\n",
    "\n",
    "1. **No In-Place Operations on Attributes:**\n",
    "   The algorithm must not include methods that perform in-place operations on its attributes.\n",
    "\n",
    "```python\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self, ...):\n",
    "        self.pop = torch.rand(10, 10)  # Attribute of the algorithm\n",
    "\n",
    "    def step_in_place(self):  # Method with in-place operations\n",
    "        self.pop.copy_(pop)\n",
    "\n",
    "    def step_out_of_place(self):  # Method without in-place operations\n",
    "        self.pop = pop\n",
    "```\n",
    "\n",
    "2. **Avoid Python Control Flow:**\n",
    "   The code logic must not rely on Python control flow structures. To handle Python control flow, use [`TracingCond`](#TracingCond), [`TracingWhile`](#TracingWhile), and [`TracingSwitch`](#TracingSwitch).\n",
    "\n",
    "```python\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self, ...):\n",
    "        self.pop = torch.rand(10, 10)\n",
    "\n",
    "    def plus(self, y):\n",
    "        self.pop += y\n",
    "\n",
    "    def minus(self, y):\n",
    "        self.pop -= y\n",
    "\n",
    "    def step_with_python_control_flow(self, y):  # Function with Python control flow\n",
    "        x = torch.rand()\n",
    "        if x > 0.5:\n",
    "            self.plus(y)\n",
    "        else:\n",
    "            self.minus(y)\n",
    "\n",
    "    def step_without_python_control_flow(self, y):  # Function without Python control flow\n",
    "        x = torch.rand()\n",
    "        cond = x > 0.5\n",
    "        _if_else_ = TracingCond(self.plus, self.minus)\n",
    "        _if_else_.cond(cond, y)\n",
    "        self.pop = pop\n",
    "```\n",
    "\n",
    "3. **Avoid In-Place Operations on `self`:**\n",
    "   In-place operations on `self` are not well-defined and cannot be compiled. Since, in tracing mode, variables outside the method may be incorrectly interpreted as static variables, use state to track them.\n",
    "\n",
    "```python\n",
    "@jit_class\n",
    "class ExampleAlgorithm(Algorithm):\n",
    "    def __init__(self, pop_size, ...):\n",
    "        super().__init__()\n",
    "        self.pop = torch.rand(pop_size, pop_size)\n",
    "\n",
    "    def strategy_1(self):  # One update strategy\n",
    "        new_pop = self.pop * self.pop\n",
    "        self.pop = new_pop\n",
    "\n",
    "    def strategy_2(self):  # Another update strategy\n",
    "        new_pop = self.pop + self.pop\n",
    "        self.pop = new_pop\n",
    "\n",
    "    def step(self):\n",
    "        control_number = torch.rand()\n",
    "        if control_number < 0.5:  # Conditional control\n",
    "            self.strategy_1()\n",
    "        else:\n",
    "            self.strategy_2()\n",
    "\n",
    "    @trace_impl(step)  # Rewrite step function for vmap support\n",
    "    def trace_step_without_operations_to_self(self):\n",
    "        pop = torch.rand(self.pop_size, self.dim, dtype=self.lb.dtype, device=self.lb.device)\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        pop = pop * self.hp[0]\n",
    "        control_number = torch.rand()\n",
    "        cond = control_number < 0.5\n",
    "        branches = (self.strategy_1, self.strategy_2)\n",
    "        state, names = self.prepare_control_flow(*branches)  # Utilize state to track self.pop\n",
    "        _if_else_ = TracingCond(*branches)\n",
    "        state = _if_else_.cond(state, cond, pop)\n",
    "        self.after_control_flow(state, *names)\n",
    "\n",
    "    @trace_impl(step)\n",
    "    def trace_step_with_operations_to_self(self):\n",
    "        pop = torch.rand(self.pop_size, self.dim, dtype=self.lb.dtype, device=self.lb.device)\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        pop = pop * self.hp[0]\n",
    "        control_number = torch.rand()\n",
    "        cond = control_number < 0.5\n",
    "        _if_else_ = TracingCond(cond)\n",
    "```\n",
    "\n",
    "### Utilizing `use_state`\n",
    "\n",
    "[`use_state`](#use_state) transforms a given stateful function (which performs in-place alterations on `nn.Module`s) into a pure-functional version that receives an additional `state` parameter (of type `Dict[str, torch.Tensor]`) and returns the altered state.\n",
    "\n",
    "The input `func` is the stateful function to be transformed or its generator function, and `is_generator` specifies whether `func` is a function or a function generator (e.g., a lambda that returns the stateful function). It defaults to `True`.\n",
    "\n",
    "Here is a simple example:\n",
    "\n",
    "```python\n",
    "@jit_class\n",
    "class Example(ModuleBase):\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.sub_mod = nn.Module()\n",
    "        self.sub_mod.buf = nn.Buffer(torch.zeros(()))\n",
    "\n",
    "    def h(self, q: torch.Tensor) -> torch.Tensor:\n",
    "        if q.flatten()[0] > self.threshold:\n",
    "            x = torch.sin(q)\n",
    "        else:\n",
    "            x = torch.tan(q)\n",
    "        x += self.g(x).abs()\n",
    "        x *= x.shape[1]\n",
    "        self.sub_mod.buf = x.sum()\n",
    "        return x\n",
    "\n",
    "    @trace_impl(h)\n",
    "    def th(self, q: torch.Tensor) -> torch.Tensor:\n",
    "        x += self.g(x).abs()\n",
    "        x *= x.shape[1]\n",
    "        self.sub_mod.buf = x.sum()\n",
    "        return x\n",
    "\n",
    "    def g(self, p: torch.Tensor) -> torch.Tensor:\n",
    "        x = torch.cos(p)\n",
    "        return x * p.shape[0]\n",
    "\n",
    "fn = use_state(lambda: t.h, is_generator=True)\n",
    "jit_fn = jit(fn, trace=True, lazy=True)\n",
    "results = jit_fn(fn.init_state(), torch.rand(10, 1))\n",
    "print(results)  # ({\"self.sub_mod.buf\": torch.Tensor(5.6)}, torch.Tensor([[0.56], ...]))\n",
    "\n",
    "# IN-PLACE update all relevant variables using the given state\n",
    "fn.set_state(results[0])\n",
    "```\n",
    "\n",
    "### Utilizing `core._vmap_fix`\n",
    "\n",
    "The module [`_vmap_fix`](#_vmap_fix) provides useful functions. After the automatic import, `_vmap_fix` enables `torch.vmap` to be correctly traced by `torch.jit.trace`, while resolving issues such as random number handling that couldn't be properly traced during the `vmap` process. It also provides the `debug_print` function, which allows dynamic printing of Tensor values during both `vmap` and tracing.For example:\n",
    "\n",
    "- [`batched_random`](#batched_random) generates a batched tensor of random values by applying the given function to the size extended with the current vmap batch size.\n",
    "- [`align_vmap_tensor`](#align_vmap_tensor) aligns a tensor with the batching dimensions of a current batched tensor.\n",
    "\n",
    "Detailed information can be found in the [`_vmap_fix`](#_vmap_fix) documentation.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
