{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Supervised Learning Tasks with Neuroevolution in EvoX "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EvoX provides solutions for supervised learning tasks based on neuroevolution, with key modules including [`SupervisedLearningProblem`](#evox.problems.neuroevolution.SupervisedLearningProblem) and [`ParamsAndVector`](#evox.utils.parameters_and_vector.ParamsAndVector). Taking the MNIST classification task as an example, this section illustrates the neuroevolution process for supervised learning by adopting the modules of EvoX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic component imports and device configuration serve as the essential starting steps for the neuroevolution process.\n",
    "\n",
    "Here, to ensure the reproducibility of results, a random seed can be optionally set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from evox.utils import ParamsAndVector\n",
    "from evox.core import Algorithm, Mutable, Parameter, jit_class\n",
    "from evox.problems.neuroevolution.supervised_learning import SupervisedLearningProblem\n",
    "from evox.algorithms import PSO\n",
    "from evox.workflows import EvalMonitor, StdWorkflow\n",
    "\n",
    "\n",
    "# Set device\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set random seed\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, a sample convolutional neural network (CNN) model is directly defined upon the PyTorch framework and then loaded onto the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of model parameters: 412\n"
     ]
    }
   ],
   "source": [
    "class SampleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SampleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 3, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(3, 3, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(3, 3, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 3, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Flatten(), nn.Linear(12, 10))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = SampleCNN().to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of model parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting dataset implies the selection of the task. The data loader now needs to be initialized based on PyTorch's built-in support.\n",
    "Here, the package `torchvision` must be installed in advance depending on your PyTorch version, if it is not already available.\n",
    "\n",
    "In case the MNIST dataset is not already present in the `data_root` directory, the `download=True` flag is set to ensure that the dataset will be automatically downloaded. Therefore, the setup may take some time during the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 3.37MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 127kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:06<00:00, 243kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.62MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "\n",
    "data_root = \"./data\" # Choose a path to save dataset\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=None,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accelerate subsequent processes, all MNIST data are pre-loaded for faster execution. Below, three datasets are pre-loaded for different stages &ndash; gradient descent training, neuroevolution fine-tuning, and model testing.\n",
    "\n",
    "It should be noted that this is an optional operation that trades space for time. Its adoption depends on your GPU capacity, and it will always take some time to prepare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for gradient descent training process\n",
    "pre_gd_train_loader = tuple([\n",
    "    (inputs.to(device), labels.to(device)) for inputs, labels in train_loader\n",
    "])\n",
    "\n",
    "# Used for neuroevolution fine-tuning process\n",
    "pre_ne_train_loader = tuple([\n",
    "    (\n",
    "        inputs.to(device),\n",
    "        labels.type(torch.float).unsqueeze(1).repeat(1, 10).to(device),\n",
    "    )\n",
    "    for inputs, labels in train_loader\n",
    "])\n",
    "\n",
    "# Used for model testing process\n",
    "pre_test_loader = tuple([\n",
    "    (inputs.to(device), labels.to(device)) for inputs, labels in test_loader\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a `model_test` function is pre-defined to simplify the evaluation of the model's prediction accuracy on the test dataset during subsequent stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model: nn.Module, data_loader: torch.utils.data.DataLoader, device: torch.device) -> float:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs: torch.Tensor = inputs.to(device=device, non_blocking=True)\n",
    "            labels: torch.Tensor = labels.to(device=device, non_blocking=True)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            _, predicted = torch.max(logits.data, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Training (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent based model training is performed first. In this example, this training is adopted to initialize the model, preparing it for subsequent neuroevolution processes. \n",
    "\n",
    "The model training process in PyTorch is compatible with neuroevolution in EvoX, making it convenient to reuse the same model implementation for further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(\n",
    "    model: nn.Module,\n",
    "    data_loader: torch.utils.data.DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    max_epoch: int,\n",
    "    device: torch.device,\n",
    "    print_frequent: int = -1,\n",
    ") -> nn.Module:\n",
    "    model.train()\n",
    "    for epoch in range(max_epoch):\n",
    "        running_loss = 0.0\n",
    "        for step, (inputs, labels) in enumerate(data_loader, start=1):\n",
    "            inputs: torch.Tensor = inputs.to(device=device, non_blocking=True)\n",
    "            labels: torch.Tensor = labels.to(device=device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if print_frequent > 0 and step % print_frequent == 0:\n",
    "                print(f\"[Epoch {epoch:2d}, step {step:4d}] \" f\"running loss: {running_loss:.4f} \")\n",
    "                running_loss = 0.0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch  0, step  500] running loss: 395.5136 \n",
      "[Epoch  1, step  500] running loss: 230.1449 \n",
      "[Epoch  2, step  500] running loss: 208.1124 \n",
      "Accuracy after gradient descent training: 89.4100 %.\n"
     ]
    }
   ],
   "source": [
    "model_train(\n",
    "    model,\n",
    "    data_loader=pre_gd_train_loader,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-2),\n",
    "    max_epoch=3,\n",
    "    device=device,\n",
    "    print_frequent=500,\n",
    ")\n",
    "\n",
    "gd_acc = model_test(model, pre_test_loader, device)\n",
    "print(f\"Accuracy after gradient descent training: {gd_acc:.4f} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuroevolution Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the pre-trained model from the previous gradient descent process, neuroevolution is progressively applied to fine-tune the model.\n",
    "\n",
    "First, the [`ParamsAndVector`](#evox.utils.parameters_and_vector.ParamsAndVector) component is used to flatten the weights of the pre-trained model into a vector, which serves as the initial center individual for the subsequent neuroevolution process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter = ParamsAndVector(dummy_model=model)\n",
    "model_params = dict(model.named_parameters())\n",
    "pop_center = adapter.to_vector(model_params)\n",
    "lower_bound = pop_center - 0.01\n",
    "upper_bound = pop_center + 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In case of algorithms specifically designed for neuroevolution, which can directly accept a dictionary of batched parameters as input, the usage of [`ParamsAndVector`](#evox.utils.parameters_and_vector.ParamsAndVector) can be unnecessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, a sample criterion is defined. Here, both the loss and accuracy of the individual model are selected and weighted to serve as the fitness function in the neuroevolution process. This step is customizable to suit the optimization direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccuracyCriterion(nn.Module):\n",
    "    def __init__(self, data_loader):\n",
    "        super().__init__()\n",
    "        data_loader = data_loader\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "        correct = (predicted == labels[:, 0]).sum()\n",
    "        fitness = -correct\n",
    "        return fitness\n",
    "\n",
    "acc_criterion = AccuracyCriterion(pre_ne_train_loader)\n",
    "loss_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "class WeightedCriterion(nn.Module):\n",
    "    def __init__(self, loss_weight, loss_criterion, acc_weight, acc_criterion):\n",
    "        super().__init__()\n",
    "        self.loss_weight = loss_weight\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.acc_weight = acc_weight\n",
    "        self.acc_criterion = acc_criterion\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        weighted_loss = self.loss_weight * loss_criterion(logits, labels)\n",
    "        weighted_acc = self.acc_weight * acc_criterion(logits, labels)\n",
    "        return weighted_loss + weighted_acc\n",
    "\n",
    "\n",
    "weighted_criterion = WeightedCriterion(\n",
    "    loss_weight=0.5,\n",
    "    loss_criterion=loss_criterion,\n",
    "    acc_weight=0.5,\n",
    "    acc_criterion=acc_criterion,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, similar to the gradient descent training and model testing processes, the neuroevolution fine-tuning process is also encapsulated into a function for convenient use in subsequent stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def neuroevolution_process(\n",
    "    workflow: StdWorkflow, \n",
    "    adapter: ParamsAndVector, \n",
    "    model: nn.Module, \n",
    "    test_loader: torch.utils.data.DataLoader, \n",
    "    device: torch.device, \n",
    "    best_acc: float, \n",
    "    max_generation: int = 2,\n",
    ") -> None:\n",
    "    for index in range(max_generation):\n",
    "        print(f\"In generation {index}:\")\n",
    "        t = time.time()\n",
    "        workflow.step()\n",
    "        print(f\"\\tTime elapsed: {time.time() - t: .4f}(s).\")\n",
    "\n",
    "        monitor = workflow.get_submodule(\"monitor\")\n",
    "        print(f\"\\tTop fitness: {monitor.topk_fitness}\")\n",
    "        best_params = adapter.to_params(monitor.topk_solutions[0])\n",
    "        model.load_state_dict(best_params)\n",
    "        acc = model_test(model, test_loader, device)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "        print(f\"\\tBest accuracy: {best_acc:.4f} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population-Based Neuroevolution Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the population-based algorithm for neuroevolution is tested first, using Particle Swarm Optimization ([PSO](#evox.algorithms.pso_variants.PSO)) as a representation. The configuration for neuroevolution is similar to that of other optimization tasks &ndash; we need to define the problem, algorithm, monitor, and workflow, along with their respective `setup()` functions to complete the initialization.\n",
    "\n",
    "A key point to note here is that the population size (`POP_SIZE` in this case) needs to be initialized in **both the problem and the algorithm** to avoid potential errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP_SIZE = 100\n",
    "vmapped_problem = SupervisedLearningProblem(\n",
    "    model=model,\n",
    "    data_loader=pre_ne_train_loader,\n",
    "    criterion=weighted_criterion,\n",
    "    pop_size=POP_SIZE,\n",
    "    device=device,\n",
    ")\n",
    "vmapped_problem.setup()\n",
    "\n",
    "pop_algorithm = PSO(\n",
    "    pop_size=POP_SIZE,\n",
    "    lb=lower_bound,\n",
    "    ub=upper_bound,\n",
    "    device=device,\n",
    ")\n",
    "pop_algorithm.setup()\n",
    "\n",
    "pop_monitor = EvalMonitor(\n",
    "    topk=3,\n",
    "    device=device,\n",
    ")\n",
    "pop_monitor.setup()\n",
    "\n",
    "pop_workflow = StdWorkflow()\n",
    "pop_workflow.setup(\n",
    "    algorithm=pop_algorithm,\n",
    "    problem=vmapped_problem,\n",
    "    solution_transform=adapter,\n",
    "    monitor=pop_monitor,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upon gradient descent, the population-based neuroevolution process start. \n",
      "In generation 0:\n",
      "\tTime elapsed:  0.9102(s).\n",
      "\tTop fitness: tensor([4.1156, 5.0516, 5.2372], device='cuda:0')\n",
      "\tBest accuracy: 89.8800 %.\n",
      "In generation 1:\n",
      "\tTime elapsed:  1.3163(s).\n",
      "\tTop fitness: tensor([3.6362, 3.6640, 3.6765], device='cuda:0')\n",
      "\tBest accuracy: 89.8800 %.\n",
      "In generation 2:\n",
      "\tTime elapsed:  1.0904(s).\n",
      "\tTop fitness: tensor([3.6362, 3.6640, 3.6765], device='cuda:0')\n",
      "\tBest accuracy: 89.8800 %.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Upon gradient descent, \"\n",
    "    \"the population-based neuroevolution process start. \"\n",
    ")\n",
    "neuroevolution_process(\n",
    "    workflow=pop_workflow,\n",
    "    adapter=adapter,\n",
    "    model=model,\n",
    "    test_loader=pre_test_loader,\n",
    "    device=device,\n",
    "    best_acc=gd_acc,\n",
    "    max_generation=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-Individual Neuroveolution Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the single-individual algorithm based neuroevolution is tested. Similar to the population-based case, we need to define the problem, algorithm, monitor, and workflow, and call their respective `setup()` functions during initialization. In this case, a random search strategy is selected as the algorithm.\n",
    "\n",
    "A key point to note here is that [`SupervisedLearningProblem`](#evox.problems.neuroevolution.SupervisedLearningProblem) should be set with `pop_size=None`, and [`EvalMonitor`](#evox.workflows.EvalMonitor) should have `topk=1`, as only a single individual is being searched. A careful hyper-parameter setup helps avoid unnecessary issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_problem = SupervisedLearningProblem(\n",
    "    model=model,\n",
    "    data_loader=pre_ne_train_loader,\n",
    "    criterion=weighted_criterion,\n",
    "    pop_size=None,\n",
    "    device=device,\n",
    ")\n",
    "single_problem.setup()\n",
    "\n",
    "@jit_class\n",
    "class RandAlgorithm(Algorithm):\n",
    "    def __init__(self, lb, ub):\n",
    "        super().__init__()\n",
    "        assert lb.ndim == 1 and ub.ndim == 1, (\n",
    "            f\"Lower and upper bounds shall have ndim of 1, \" f\"got {lb.ndim} and {ub.ndim}. \"\n",
    "        )\n",
    "        assert lb.shape == ub.shape, f\"Lower and upper bounds shall have same shape, \" f\"got {lb.ndim} and {ub.ndim}. \"\n",
    "        self.hp = Parameter([1.0, 2.0])\n",
    "        self.lb = lb\n",
    "        self.ub = ub\n",
    "        self.dim = lb.shape[0]\n",
    "        self.pop = Mutable(torch.empty(1, lb.shape[0], dtype=lb.dtype, device=lb.device))\n",
    "        self.fit = Mutable(torch.empty(1, dtype=lb.dtype, device=lb.device))\n",
    "\n",
    "    def step(self):\n",
    "        pop = torch.rand(\n",
    "            self.dim,\n",
    "            dtype=self.lb.dtype,\n",
    "            device=self.lb.device,\n",
    "        )\n",
    "        pop = pop * (self.ub - self.lb)[None, :] + self.lb[None, :]\n",
    "        pop = pop * self.hp[0]\n",
    "        self.pop.copy_(pop)\n",
    "        self.fit.copy_(self.evaluate(pop))\n",
    "single_algorithm = RandAlgorithm(lb=lower_bound, ub=upper_bound)\n",
    "\n",
    "single_monitor = EvalMonitor(\n",
    "    topk=1,\n",
    "    device=device,\n",
    ")\n",
    "single_monitor.setup()\n",
    "\n",
    "single_workflow = StdWorkflow()\n",
    "single_workflow.setup(\n",
    "    algorithm=single_algorithm,\n",
    "    problem=single_problem,\n",
    "    solution_transform=adapter,\n",
    "    monitor=single_monitor,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upon gradient descent, the single-individual neuroevolution process start. \n",
      "In generation 0:\n",
      "\tTime elapsed:  0.5737(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 1:\n",
      "\tTime elapsed:  0.5409(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 2:\n",
      "\tTime elapsed:  0.4984(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 3:\n",
      "\tTime elapsed:  0.3844(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 4:\n",
      "\tTime elapsed:  0.3857(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 5:\n",
      "\tTime elapsed:  0.3844(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 6:\n",
      "\tTime elapsed:  0.3862(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 7:\n",
      "\tTime elapsed:  0.3851(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 8:\n",
      "\tTime elapsed:  0.3825(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 9:\n",
      "\tTime elapsed:  0.3840(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 10:\n",
      "\tTime elapsed:  0.3821(s).\n",
      "\tTop fitness: tensor([5.8806], device='cuda:0')\n",
      "\tBest accuracy: 89.4600 %.\n",
      "In generation 11:\n",
      "\tTime elapsed:  0.3822(s).\n",
      "\tTop fitness: tensor([4.2521], device='cuda:0')\n",
      "\tBest accuracy: 89.5000 %.\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Upon gradient descent, \"\n",
    "    \"the single-individual neuroevolution process start. \"\n",
    ")\n",
    "neuroevolution_process(\n",
    "    workflow=single_workflow,\n",
    "    adapter=adapter,\n",
    "    model=model,\n",
    "    test_loader=pre_test_loader,\n",
    "    device=device,\n",
    "    best_acc=gd_acc,\n",
    "    max_generation=12,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_evoxtorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
